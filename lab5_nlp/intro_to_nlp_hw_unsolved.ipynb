{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morhunenko hw 5 nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "YfeVtEQwRmsR",
    "outputId": "47ad1459-0da2-44dd-f1f0-550b63c8cc06"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "!wget -O surnames.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ji7dhr9FojPeV51dDlKRERIqr3vdZfhu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKoTq9xW-PdW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q5wMjxQMeQS"
   },
   "source": [
    "# Recurrent Neural Networks, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY5-j_RsRzxa"
   },
   "source": [
    "## Surname Classification\n",
    "\n",
    "It's time to learn how to use RNNs - probably, the most widely-used type\n",
    "of neural networks in NLP applications. We are going to predict a language\n",
    "by a surname with its help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPPJoWEpSN_B"
   },
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "with open('surnames.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        surname, lang = line.strip().split('\\t')\n",
    "        data.append(surname)\n",
    "        labels.append(lang)\n",
    "\n",
    "for i in np.random.randint(0, len(data), 10):\n",
    "    print(data[i], labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bVJlxzYhWuf"
   },
   "source": [
    "### Warming Up\n",
    "\n",
    "Check yourself: try to predict what language the surnames are from :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7JquuckaAGb"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def test_generator():\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight('balanced', classes, labels)\n",
    "    classes = {label: ind for ind, label in enumerate(classes)}\n",
    "\n",
    "    probs = np.array([weights[classes[label]] for label in labels])\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    ind = np.random.choice(np.arange(len(data)), p=probs)\n",
    "    yield data[ind]\n",
    "    \n",
    "    while True:\n",
    "        new_ind = np.random.choice(np.arange(len(data)), p=probs)\n",
    "        yield labels[ind], data[new_ind]\n",
    "        ind = new_ind\n",
    "        \n",
    "gen = test_generator()\n",
    "question = next(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KNeNxm1hsKs"
   },
   "source": [
    "Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-Q3OSXpAY8BS"
   },
   "outputs": [],
   "source": [
    "#@title Check yourself (and the data) { run: \"auto\" }\n",
    "answer = \"Vietnamese\" #@param [\"Arabic\", \"Chinese\", \"Czech\", \"Dutch\", \"English\", \"French\", \"German\", \"Greek\", \"Irish\", \"Italian\", \"Japanese\", \"Korean\", \"Polish\", \"Portuguese\", \"Russian\", \"Scottish\", \"Spanish\", \"Vietnamese\"]\n",
    "\n",
    "correct_answer, question = next(gen)\n",
    "\n",
    "if 'correct_count' not in globals():\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "else:\n",
    "    if answer == correct_answer:\n",
    "        print('You are correct', end=' ')\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        print(\"No, it's\", correct_answer, end=' ')\n",
    "\n",
    "    total_count += 1\n",
    "    print('({} / {})'.format(correct_count, total_count))\n",
    "    \n",
    "print('Next surname:', question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Yz20u6dhll0"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfP4Sb68TWlY"
   },
   "source": [
    "First of all, we have to split the data on train and test.\n",
    "\n",
    "We have to be careful: the classes are unballanced and we should split them proportionaly. The `stratify` parameter of the `train_test_split` will help us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eE-s7q7RmAM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    data, labels, test_size=0.3, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzX5zHobUHc0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "langs = set(labels)\n",
    "\n",
    "train_distribution = Counter(labels_train)\n",
    "train_distribution = [train_distribution[lang] for lang in langs]\n",
    "\n",
    "test_distribution = Counter(labels_test)\n",
    "test_distribution = [test_distribution[lang] for lang in langs]\n",
    "\n",
    "plt.figure(figsize=(17, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(langs)), train_distribution, bar_width, align='center', alpha=0.5, label='train')\n",
    "plt.bar(np.arange(len(langs)) + bar_width, test_distribution, bar_width, align='center', alpha=0.5, label='test')\n",
    "plt.xticks(np.arange(len(langs)) + bar_width / 2, langs)\n",
    "plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgmDagAq5-mm"
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDgB4iVCWbNE"
   },
   "source": [
    "We are going to start with a baseline. **You should always start with a simple baseline.** \n",
    "\n",
    "**Task** That is, use logistic regression, Luke!\n",
    "\n",
    "https://scikit-learn.org/stable/modules/compose.html#pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLxUE4thXyV2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 4), analyzer ='char')\n",
    "classifier = LogisticRegression(solver='lbfgs', max_iter=100000)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz9ngs_lWn3p"
   },
   "source": [
    "What metrics are we going to calculate?\n",
    "\n",
    "We are dealing with multi-class classification, so we have quite a wide range of options.\n",
    "\n",
    "For instance, we can use accuracy or F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJZt8sKM6zEA"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "preds = model.predict(data_test)\n",
    "\n",
    "print('Accuracy = {:.2%}'.format(accuracy_score(labels_test, preds)))\n",
    "print('Classification report:')\n",
    "print(classification_report(labels_test, preds))\n",
    "\n",
    "assert accuracy_score(labels_test, preds) > 0.83, 'No, really, try smth better'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNRhUTNaW45M"
   },
   "source": [
    "It's not that simple with F1-score, actually. It's designed to be used in the binary classification setup.\n",
    "But we can adjust it to multi-class using:\n",
    "- weighted averaging: the one that `classification_report` outputed. It tooks into account frequencies of\n",
    "    classes and averages the scores by them.\n",
    "- micro averaging: F1-score calculated by the aggregation of the all true positives, false positives\n",
    "    and false negatives.\n",
    "- macro averaging: mean of all F1-scores of the classes.\n",
    "\n",
    "The first two are designed to deal with classes disballance, the last ignores it. Actully, it's up to\n",
    "you to decide, whether you need to predict classes equally good or the more frequent classes are more\n",
    "important to you.\n",
    "\n",
    "We'll use weighted averaging, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0SwCbWN8ZQd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "label_names = list(set(labels_test))\n",
    "confusion = confusion_matrix(labels_test, preds, labels=label_names).astype(np.float)\n",
    "confusion /= confusion.sum(axis=-1, keepdims=True)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion, cmap='Reds')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels([''] + label_names, rotation=45)\n",
    "ax.set_yticklabels([''] + label_names)\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo0lsnzV-i-P"
   },
   "source": [
    "## The Path of Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn0LqFD4_n77"
   },
   "source": [
    "### Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZWTilz0b4NZ"
   },
   "source": [
    "Let's say, we wanna apply a neural network now. What can we do based on the things we've already learned?\n",
    "\n",
    "Compare the problem with the one we tackled in the previous notebook. We had a *sequence* of words there,\n",
    "and we have *sequence* of letters here. And we have to classify the sequences in both cases.\n",
    "It's quite similar, isn't it?\n",
    "\n",
    "And, as you should have already guessed, the key ingredient in our disposal are embeddings. \n",
    "\n",
    "We can embed the letters as easily as we embedded words, you see.\n",
    "\n",
    "Now, let's read the sequence the way you read it. Like, symbol-by-symbol:  \n",
    "![](https://i.ibb.co/hM2pxJq/RNN-prediction.png)\n",
    "\n",
    "The information we are reading can be stored in some special memory cell (a simple vector).\n",
    "For instance, the information that the word contains `ó` is meaningful enough to remember it\n",
    "during the whole reading process. Also `ski` is quite Polish suffix to simply forget about it. But we can see only one symbol at any moment, which means we should store also the information about possibility of the existance of the suffix `ski` in the word (its confidence is shown by different shades of green in the picture).\n",
    "\n",
    "The final memory cell stores all interesting information about the word, so we can perform classification\n",
    "on top of it.\n",
    "\n",
    "And well, that is. We almost invented recurrent neural networks! \n",
    "\n",
    "The idea behind RNNs is following: read given sequence sequentially and apply the same operation\n",
    "(that's why it's called *recurrent*) to every element of the sequence (and current memory cell)\n",
    "to obtain new memory cell. The arrows in the picture show the operation of rewriting the memory.\n",
    "All learnable parameters of the RNN are somewhere inside them.\n",
    "\n",
    "![](https://i.ibb.co/0GPj9Zt/Rnn.png)\n",
    "\n",
    "The picture above shows the learnable function as $f(x_t, h_{t-1})$, where $x_t$ is the embedding\n",
    "of the current element and $h_{t-1}$ is memory cell vector obtained from the previous step.\n",
    "\n",
    "What function can be used?\n",
    "\n",
    "The simplest choice is summation: we can sum all input embeddings. And we actually did something\n",
    "similar in our word2vec, remember?\n",
    "\n",
    "Classical recurrent unit applies the following function:\n",
    "$$f(x_t, h_{t-1}) = tanh(W_h [h_{t-1}; x_t] + b_h),$$\n",
    "$$h_t = f(x_t, h_{t-1}),$$\n",
    "\n",
    "where $[h_{t-1}; x_t]$ is simple concatenation of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPYrTVACW8SM"
   },
   "source": [
    "Let's implement it! And to test it, let's apply it to a simple problem: memorization of the first element\n",
    "in the sequence.\n",
    "E.g., for the sequence `[1, 2, 1, 3]` correct output is `1`.\n",
    "\n",
    "The batch generation function is following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOI4JGgHT-z3"
   },
   "outputs": [],
   "source": [
    "def generate_data(batch_size=128, seq_len=5):\n",
    "    data = torch.randint(0, 10, size=(seq_len, batch_size), dtype=torch.long)\n",
    "    return data, data[0]\n",
    "\n",
    "\n",
    "X_val, y_val = generate_data()\n",
    "X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ0Gsr4SFNtB"
   },
   "source": [
    "*NB.* Pay attention to the batch's dimensions: `(sequence_length, batch_size, input_size)`.\n",
    "All `RNN` cells in pytorch work with such format by default (but you can change the behaviour using\n",
    "the argument `batch_first`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqS7HPRhZSBC"
   },
   "source": [
    "**Task** Implement the `SimpleRNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed1b2TUvZRs0"
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNNCell(self.input_size, self._hidden_size, bias=True)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        # inputs is a tensor in the time major format\n",
    "        seq_len, batch_size = inputs.shape[:2]\n",
    "        if hidden is None:\n",
    "            # Initialize hidden with zeros\n",
    "            hidden = inputs.new_zeros((batch_size, self._hidden_size))\n",
    "\n",
    "        hiddens = []\n",
    "\n",
    "        for input_el in inputs:\n",
    "            hiden_el = self.rnn(input_el,hidden)\n",
    "            hiddens.append(hiden_el)\n",
    "\n",
    "        return torch.stack(hiddens), hiden_el\n",
    "\n",
    "sequence_length = 32\n",
    "batch_size = 64\n",
    "input_size = 10\n",
    "hidden_size = 16\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size)\n",
    "\n",
    "input_tensor = torch.randn((sequence_length, batch_size, input_size))\n",
    "\n",
    "outputs = rnn(input_tensor)\n",
    "\n",
    "assert isinstance(outputs, (list, tuple)), 'You should return both all hidden states tensor and the last hidden state'\n",
    "assert len(outputs) == 2\n",
    "assert outputs[0].shape == (sequence_length, batch_size, hidden_size)\n",
    "assert outputs[1].shape == (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS2xw2YIZ_EU"
   },
   "source": [
    "Guess, it should more obvious why RNNs use so called time-major format by default: `inputs[i]`\n",
    "slice is contiguous in memory, while `inputs[:, i]` is not. It's much faster to perform matrix\n",
    "multiplications on contiguous memory chunks.\n",
    "A RNN cell with `batch_first=True` parameter is simply perform batch transposition twice: before and\n",
    "after RNN operations.\n",
    "\n",
    "**Task** Implement `MemorizerModel` using a sequence of `Embedding -> SimpleRNN -> Linear` operations.\n",
    "(Remember, you can use `nn.Sequential` for such simple modules).\n",
    "\n",
    "Let's use one-hot-encoding embeddings. That means we can use `torch.eye(N)` matrix to initialize\n",
    "embeddings (pass it to `nn.Embedding.from_pretrained` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEUr4Xa9Z81I"
   },
   "outputs": [],
   "source": [
    "class MemorizerModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding.from_pretrained(torch.eye(10))\n",
    "        \n",
    "        self._rnn = SimpleRNN(10, hidden_size)\n",
    "        \n",
    "        self._lin = nn.Linear(hidden_size, 10)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._emb(inputs)\n",
    "        x = self._rnn(x)[1]\n",
    "        x = self._lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiDRoQWDawaW"
   },
   "source": [
    "Simply run the training cycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbVk7zUjUQ_v"
   },
   "outputs": [],
   "source": [
    "rnn = MemorizerModel(hidden_size=32)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters())\n",
    "\n",
    "total_loss = 0\n",
    "epochs_count = 1000\n",
    "max_seq_len = 10\n",
    "for epoch_ind in range(epochs_count):\n",
    "    X_train, y_train = generate_data(seq_len=np.random.randint(1, max_seq_len))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    rnn.train()\n",
    "    \n",
    "    logits = rnn(X_train)\n",
    "    loss = criterion(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if (epoch_ind + 1) % 100 == 0:\n",
    "        rnn.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = rnn(X_val)\n",
    "            val_loss = criterion(logits, y_val)\n",
    "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
    "                                                             total_loss / 100, val_loss.item()))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1FMf4O3XL41"
   },
   "source": [
    "And evalutation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwFIIihP5Iw_"
   },
   "outputs": [],
   "source": [
    "X_val, y_val = generate_data(seq_len=50)\n",
    "\n",
    "logits = rnn(X_val)\n",
    "loss = criterion(logits, y_val)\n",
    "\n",
    "print('Accuracy = {}, Loss = {}'.format(((logits.argmax(-1) == y_val).float().sum() / y_val.shape[0]).item(), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQJg3FROIq4v"
   },
   "source": [
    "You see, the network was trained to work with sequences with length up to 10 elements, but it's quite successful with a longer sequences to (well, it's not like the task was too hard, but still, impressive, isn't it?)\n",
    "\n",
    "**Task** Try to vary the `max_seq_len` value. Check the model performance.\n",
    "\n",
    "The expected behaviour is following: with `max_seq_len == 30` network already wouldn't be able to learn anything (and to work with a sequence with length equal to 50). It may seem to be counterintuitive: we are feeding the model with samples more similar to the target samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSHNuT5b61Ky"
   },
   "source": [
    "### RNNs' Training Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOhYHBFzhoXu"
   },
   "source": [
    "![bptt](https://image.ibb.co/cEYkw9/rnn_bptt_with_gradients.png)  \n",
    "*From [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)*\n",
    "\n",
    "To understand the source of the obstacle you encountered in the previous section, you have to learn more about RNNs training. Check these posts: [Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) and [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/).\n",
    "\n",
    "There are two problems actually: *vanishing gradients* and *exploding gradients*. We just faced the first one: the output in our toy problem depended on the first element, so the gradient shouldn't vanish on the way back from last to first element. And when it didn't, the model'd successfully learned to memorize the first element (and the length of sequence it had been applied to mattered only a little). But when gradients vanished, model couldn't learn anything!\n",
    "\n",
    "Another problem is *exploding gradients*. It has same source: repeated gradient multiplications on the backward pass. You should use gradients clipping more or less always to fight it. Just call `nn.utils.clip_grad_norm_(rnn.parameters(), 1.)` before optimizer's `step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBnrtvlojWIN"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nqgxsw38ju8F"
   },
   "source": [
    "We are almost ready to solve the task using our `SimpleRNN`.\n",
    "\n",
    "Let's collect some statistics from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7-fQPwJUKtV"
   },
   "outputs": [],
   "source": [
    "symbols = set(symb for word in data_train for symb in word)\n",
    "char2ind = {symb: ind + 1 for ind, symb in enumerate(symbols)}\n",
    "char2ind['<pad>'] = 0\n",
    "ind2char = [symb for symb, _ in sorted(char2ind.items(), key=lambda pair: pair[1])]\n",
    "\n",
    "lang2ind = {lang: ind for ind, lang in enumerate(set(labels_train))}\n",
    "\n",
    "max_word_len = max(len(word) for word in data_train)\n",
    "\n",
    "print('Chars index:', char2ind)\n",
    "print('Langs index:', lang2ind)\n",
    "print('Max word length:', max_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcFgEy7YeFw0"
   },
   "source": [
    "... and convert the dataset to a format understandable by a neural network.\n",
    "\n",
    "**Task** Convert the data and implement the batches genetator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DonuNQRPkPDQ"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, labels, char2ind, label2ind):\n",
    "#     <convert the data and labels to numpy matrices>\n",
    "\n",
    "\n",
    "X_train, y_train = convert_data(data_train, labels_train, char2ind, lang2ind)\n",
    "X_test, y_test = convert_data(data_test, labels_test, char2ind, lang2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3EUbkG4kMJ6"
   },
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train) == len(data_train)\n",
    "assert len(X_test) == len(y_test) == len(data_test)\n",
    "assert len(y_train.shape) == 1, 'Labels should be stored in a numpy array'\n",
    "assert isinstance(X_train[0], list) and isinstance(X_train[0][0], int), 'Store lists of indices in the data array'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqvp3azB44c3"
   },
   "source": [
    "**Task** Finish the batch iterator function implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWWClVsTRVuA"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    num_samples = len(X)\n",
    "\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        \n",
    "        batch_indices = indices[start: end]\n",
    "        \n",
    "        <concatenate everything to tensors>\n",
    "            \n",
    "        yield {\n",
    "            'tokens': LongTensor(X_batch), \n",
    "            'lengths': LongTensor(lengths),\n",
    "            'labels': LongTensor(y_batch)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD5i7WmTVlGk"
   },
   "outputs": [],
   "source": [
    "batch = next(iterate_batches((X_train, y_train), batch_size=8))\n",
    "\n",
    "assert isinstance(batch, dict)\n",
    "assert isinstance(batch['tokens'], LongTensor) and batch['tokens'].shape[1] == 8\n",
    "assert isinstance(batch['lengths'], LongTensor) and batch['lengths'].shape == (8,)\n",
    "assert isinstance(batch['labels'], LongTensor) and batch['labels'].shape == (8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftvZJ7xGjaLs"
   },
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnkAUetgs6Tr"
   },
   "source": [
    "**Task** Implement a simple model based on `SimpleRNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zk3OSidVS_px"
   },
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, rnn_hidden_dim, classes_count):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        <add layers there>\n",
    "            \n",
    "    def forward(self, tokens, lengths, **kwargs):\n",
    "        \"\"\"\n",
    "        Applies embed(tokens, lengths) to obtain word embeddings \n",
    "         and applies output nn.Linear layer to classify them\n",
    "        \"\"\"\n",
    "    \n",
    "    def embed(self, tokens, lengths):\n",
    "        \"\"\"\n",
    "        Builds character-level word embeddings for the given tokens\n",
    "        (use the last rnn's state)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vXN-QIrZs95"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(len(data) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, batch in enumerate(iterate_batches((data, labels), batch_size)):\n",
    "            logits = model(**batch)\n",
    "            loss = criterion(logits, batch['labels'])\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}'.format(i, batchs_count, loss.item()), end='')\n",
    "                \n",
    "    return epoch_loss / batchs_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9vBDF2gbypR"
   },
   "outputs": [],
   "source": [
    "model = SurnamesClassifier(vocab_size=len(char2ind), emb_dim=32, rnn_hidden_dim=128, classes_count=len(lang2ind))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=10, batch_size=32, train_data=(X_train, y_train),\n",
    "    val_data=(X_test, y_test), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2AADFuk_0Pl"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uqQNDDk-o8A"
   },
   "source": [
    "**Task** Check the model, try to predict the language for your own surname:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWXnC-7r-0x5"
   },
   "outputs": [],
   "source": [
    "# Do NOT forget to turn on eval mode\n",
    "model.eval()\n",
    "\n",
    "# You don't need to do backprop, so...\n",
    "with torch.no_grad():\n",
    "    <your code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBzMErFq9Ash"
   },
   "source": [
    "**Task** Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvA2rk6D75HJ"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    <collect y_true & y_pred>\n",
    "    \n",
    "    assert y_test.shape[0] == len(y_true) == len(y_pred)\n",
    "        \n",
    "    print('Accuracy = {:.2%}'.format(accuracy_score(y_true, y_pred)))\n",
    "    print('Classification report:')\n",
    "    print(classification_report(\n",
    "        y_true, y_pred, target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]\n",
    "    ))\n",
    "    \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQyTOHbCuRBL"
   },
   "outputs": [],
   "source": [
    "assert evaluate_model(model, X_test, y_test) > 0.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRABfnouv8u3"
   },
   "source": [
    "## The Curious Case of the Lost Percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhLH8dK0nq7M"
   },
   "source": [
    "### On RNNs Output States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMxFuSY5nxrU"
   },
   "source": [
    "'What the heck, why the quality is so low?', you're asking me. Well, the main thing is: we are not using the RNN output states properly. Still. It's time to fix it.\n",
    "\n",
    "You see, the output states of a RNN (the first output our `SimpleRNN`) look this way:  \n",
    "![](https://i.ibb.co/rw6CQw0/RNN-outputs.png)\n",
    "\n",
    "It's a 3D tensor `(sequence_length, batch_size, hidden_size)`.\n",
    "\n",
    "We are using the last slice of the tensor to classify the sequence. But wait, the sequences in our batch have different lengths! Most of them are shorter and filled with paddings in the end (the white cells in the picture).\n",
    "\n",
    "So, basically, we are using wrong states to classify the sequence. Of course, they should have contained the information about the sequence, but... The network might easily forget everything as we just saw in our toy `MemorizerModel` example.\n",
    "\n",
    "Let's deal with it. Let's slice the tensor to obtain the last state of the sequences.\n",
    "\n",
    "We'll start with a simpler tensor to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytKSlqqTslab"
   },
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "batch_size = 4\n",
    "\n",
    "data = torch.arange(20).view(sequence_length, batch_size)\n",
    "lengths = torch.tensor([3, 2, 5, 4])\n",
    "\n",
    "data, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n-B6trutj-g"
   },
   "source": [
    "**Task** Use a single slice operation to get the `last_states`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OACXhoItEtg"
   },
   "outputs": [],
   "source": [
    "last_states = data[lengths - 1, torch.arange(4)]\n",
    "\n",
    "assert torch.all(last_states == torch.tensor([8, 5, 18, 15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlTA7UsOtqHc"
   },
   "source": [
    "**Task** Use the same operation to get correct last states in the `embed` method. Train network and compare its performance with the old result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CExmuRZduf4J"
   },
   "outputs": [],
   "source": [
    "<train the model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_gsFN4iufEx"
   },
   "outputs": [],
   "source": [
    "assert evaluate_model(model, X_test, y_test) > 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13x5erUgTjDC"
   },
   "source": [
    "### LSTM & GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAjZh9YkYAMH"
   },
   "source": [
    "Guess, it's time to try something more substantial to beat logistic regression. We've already learned that RNNs' weak point is their forgetfulness. And vanishing gradients are at the core of it.\n",
    "\n",
    "There is a way to deal with it - so called *gates*. The idea behind a *gate* is simple: we should give our network a mechanism to control its own memory.\n",
    "\n",
    "You see, our `SimpleRNN` overrides its own memory vector $h_{t-1}$ every time it meets a new input $x_t$. In a way, it's like an amnesiac fish:  \n",
    "![](https://i.ibb.co/WVJXT2t/bhwrog1.jpg)\n",
    "\n",
    "Let's generate a vector $g \\in \\{0,1\\}^n$ that will define which elements in $h_{t-1}$ are useful and which should be updated by the new came information:\n",
    "$$h_t = g \\odot f(x_t, h_{t-1}) + (1 - g) \\odot h_{t-1},$$ where $ \\odot$ is elementwise multiplication.\n",
    "\n",
    "For instance,\n",
    "$$\n",
    " \\begin{bmatrix}\n",
    "  8 \\\\\n",
    "  11 \\\\\n",
    "  3 \\\\\n",
    "  7\n",
    " \\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  0\n",
    " \\end{bmatrix}\n",
    " \\odot\n",
    "  \\begin{bmatrix}\n",
    "  7 \\\\\n",
    "  11 \\\\\n",
    "  6 \\\\\n",
    "  5\n",
    " \\end{bmatrix}\n",
    " +\n",
    "  \\begin{bmatrix}\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  1\n",
    " \\end{bmatrix}\n",
    " \\odot\n",
    "  \\begin{bmatrix}\n",
    "  8 \\\\\n",
    "  5 \\\\\n",
    "  3 \\\\\n",
    "  7\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And we need a differentiable function, so instead of $g \\in {0, 1}$ we'll use $\\sigma(q(x_t, h_{t-1})) \\in [0, 1]$.\n",
    "\n",
    "As a result, the network should learn itself to choose what to remember.\n",
    "\n",
    "The first architecture successfully applied this idea was LSTM (Long Short-Term Memory).\n",
    "\n",
    "Well, the first difference is that memory state consists of two vectors: $[h_t, c_t]$.\n",
    "\n",
    "The computations look a bit more complicated that I sketched them above.\n",
    "\n",
    "The first step looks similar to vanilla RNN computations:\n",
    "$$\\tilde c_{t} = tanh(W_h [h_{t-1}; x_t] + b_h)$$\n",
    "\n",
    "Remember, in our implementation we used the vector as the next memory state. But now we are going to mix the $\\tilde c_t$ with old memory state $c_{t-1}$.  How? That's exactly where the gates come to play!\n",
    "\n",
    "We'll predict two values:\n",
    "$$f = \\sigma(W_f [h_{t-1}; x_t] + b_f),$$\n",
    "$$i = \\sigma(W_i [h_{t-1}; x_t] + b_i).$$\n",
    "\n",
    "The first one we are going to use as a *forget* gate (which parts we wanna forget). The second is *input* gate (which parts of the new state are the most useful):\n",
    "$$c_t = f \\odot c_{t-1} + i \\odot \\tilde c_t.$$\n",
    "\n",
    "We got our new memory state! Now let's decide, what to output:\n",
    "$$o = \\sigma(W_o [h_{t-1}; x_t] + b_o),$$\n",
    "$$h_t = o \\odot tanh(c_t).$$\n",
    "\n",
    "$h_t$ is (again) the outputed state of the cell. Cell is looking at the memory state and deciding what should be shown to the world\n",
    "\n",
    "The whole process in one picture:\n",
    "\n",
    "![](https://image.ibb.co/e6HQUU/details.png)  \n",
    "*From [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/)*\n",
    "\n",
    "Why does the thing help anyhow? Look at the gradient $\\frac{\\partial c_t}{\\partial c_{t-1}}$. It is proportional to the forget gate $f$. When $f$ is equal to one, gradients are unchanged. So it's possible to learn not to forget anything (as well as forget useless things).\n",
    "\n",
    "To gain a better insight check this wonderful post: [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "**Task** The theory may seem to be confusing, but there is nothing simpler than try it in practice. Just replace `SimpleRNN` with `nn.LSTM` module. Do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vhJlbVT5q7e"
   },
   "outputs": [],
   "source": [
    "<train a model with LSTM RNN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfqLzNyq84DV"
   },
   "outputs": [],
   "source": [
    "assert evaluate_model(model, X_test, y_test) > 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DickfCNwJZFT"
   },
   "source": [
    "## The Final Quest\n",
    "\n",
    "Let's beat our logistic regression baseline!\n",
    "\n",
    "**Task** A common RNNs' extension is *Bidirectional* RNN. Basically, it's two RNNs, one (forward) reads sequence left to right, another (backward) - right to left.\n",
    "\n",
    "The result hidden state is $h_t = [f_t; b_t]$. You see, the $f_t$ is context representation of the first $t$ elements, the $b_t$ is context embedding of the last $T-t$ elements. Which means that $h_t$ contains information about the whole sequence.\n",
    "\n",
    "Use `LSTM`'s' parameter `bidirectional=True` to turn the mode on.\n",
    "\n",
    "In this case you should probably use [`PackedSequence`](https://pytorch.org/docs/stable/nn.html#packedsequence) to obtain a correct `last_state`. It's just a handy alternative to the way we implemented our `last_state` extraction (Oh, did you really think it's not in the library already?).\n",
    "\n",
    "Simply call `pack_padded_sequence` to create `PackedSequence` and call `pad_packed_sequence` to obtain an ordinary tensor back:\n",
    "\n",
    "```python\n",
    "embs = nn.utils.rnn.pack_padded_sequence(embs, lengths, enforce_sorted=False)\n",
    "states, (last_state, _) = self._rnn(embs)\n",
    "states, _ = nn.utils.rnn.pad_packed_sequence(states)\n",
    "```\n",
    "\n",
    "**Task** Add `nn.Dropout` whereever it helps. It really do help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Wxh0vpiTARv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[EN] Week 05 - RNNs (Part 1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
